{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/boom/anaconda3/envs/marigold/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of UNet2DConditionModel were not initialized from the model checkpoint at ./stable-diffusion-v1-5 and are newly initialized because the shapes did not match:\n",
      "- conv_in.weight: found shape torch.Size([320, 4, 3, 3]) in the checkpoint and torch.Size([320, 12, 3, 3]) in the model instantiated\n",
      "- conv_out.bias: found shape torch.Size([4]) in the checkpoint and torch.Size([8]) in the model instantiated\n",
      "- conv_out.weight: found shape torch.Size([4, 320, 3, 3]) in the checkpoint and torch.Size([8, 320, 3, 3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "\n",
    "from diffusers import (\n",
    "    DiffusionPipeline,\n",
    "    DDIMScheduler,\n",
    "    DDPMScheduler,\n",
    "    UNet2DConditionModel,\n",
    "    AutoencoderKL,\n",
    ")\n",
    "from diffusers.utils import BaseOutput\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "num_train_timesteps=300\n",
    "unet = UNet2DConditionModel(\n",
    ").from_pretrained('./stable-diffusion-v1-5',\n",
    "                  subfolder=\"unet\",\n",
    "                  in_channels=12,\n",
    "                  out_channels=8,\n",
    "                  low_cpu_mem_usage=False,\n",
    "                  ignore_mismatched_sizes=True,\n",
    "                  local_files_only = True)\n",
    "vae = AutoencoderKL().from_pretrained('./stable-diffusion-v1-5',subfolder=\"vae\")\n",
    "scheduler = DDIMScheduler(num_train_timesteps=1000).from_pretrained('./stable-diffusion-v1-5',subfolder=\"scheduler\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained('./stable-diffusion-v1-5',subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained('./stable-diffusion-v1-5',subfolder=\"text_encoder\")\n",
    "# device = 'cuda'\n",
    "# vae.requires_grad_(False)\n",
    "# unet.requires_grad_(True)\n",
    "# text_encoder.requires_grad_(False)\n",
    "# text_encoder.text_model.embeddings.token_embedding.requires_grad_(False)\n",
    "# # tokenizer.requires_grad_(False)\n",
    "\n",
    "# vae.eval()\n",
    "# text_encoder.text_model.embeddings.token_embedding.eval()\n",
    "# text_encoder.eval()\n",
    "# unet.train()\n",
    "\n",
    "# vae.to(device)\n",
    "# unet.to(device)\n",
    "class FinetuneUnet(nn.Module):\n",
    "    rgb_latent_scale_factor = 0.18215\n",
    "    depth_latent_scale_factor = 0.18215\n",
    "    def __init__(\n",
    "            self,\n",
    "            unet: unet,\n",
    "            vae: AutoencoderKL,\n",
    "            scheduler: DDIMScheduler,\n",
    "            text_encoder: CLIPTextModel,\n",
    "            tokenizer: CLIPTokenizer,\n",
    "    ):\n",
    "        super(FinetuneUnet, self).__init__()\n",
    "        self.unet = unet\n",
    "        self.vae = vae\n",
    "        self.scheduler = scheduler\n",
    "        self.text_encoder = text_encoder\n",
    "        self.tokenizer = tokenizer\n",
    "        self.empty_text_embed = None\n",
    "\n",
    "    def __encode_empty_text(self):\n",
    "        \"\"\"\n",
    "        Encode text embedding for empty prompt\n",
    "        \"\"\"\n",
    "        prompt = \"\"\n",
    "        text_inputs = self.tokenizer(\n",
    "            \"\",\n",
    "            padding=\"do_not_pad\",\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        text_input_ids = text_inputs.input_ids.to(self.text_encoder.device)\n",
    "        self.empty_text_embed = self.text_encoder(text_input_ids)[0].to(torch.float32)\n",
    "\n",
    "    def encode_3c(self, render: torch.Tensor) -> torch.Tensor:\n",
    "        # encode\n",
    "        h = self.vae.encoder(render)\n",
    "        moments = self.vae.quant_conv(h)\n",
    "        mean, logvar = torch.chunk(moments, 2, dim=1)\n",
    "        # scale latent\n",
    "        render_latent = mean * self.rgb_latent_scale_factor\n",
    "        return render_latent\n",
    "\n",
    "    def forward(self,render_depth_normal_latent):\n",
    "        device = render_depth_normal_latent.device\n",
    "\n",
    "        render_latent = render_depth_normal_latent[:,0:4,:,:]\n",
    "        depth_normal_latent = render_depth_normal_latent[:,4:12,:,:]\n",
    "\n",
    "        # noise is 8 channel\n",
    "        noise = torch.randn((render_latent.shape[0],8,render_latent.shape[2],render_latent.shape[3]), device=device) # noise for depth_normal_latent\n",
    "        bs = render_latent.shape[0]\n",
    "        timesteps = torch.randint(\n",
    "            0, self.scheduler.config.num_train_timesteps, (bs,), device=device,\n",
    "            dtype=torch.int64\n",
    "        )\n",
    "        # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "        noisy_depth_normal = self.scheduler.add_noise(depth_normal_latent, noise, timesteps)\n",
    "\n",
    "        # empty text embedding\n",
    "        if self.empty_text_embed is None:\n",
    "            self.__encode_empty_text()\n",
    "        batch_empty_text_embed = self.empty_text_embed.repeat(\n",
    "            (render_latent.shape[0], 1, 1)\n",
    "        )  # [B, 2, 1024]\n",
    "\n",
    "        unet_input = torch.cat([render_latent,noisy_depth_normal],dim=1)\n",
    "        pred_noise = self.unet(unet_input,timesteps,encoder_hidden_states=batch_empty_text_embed).sample\n",
    "        return pred_noise, noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet2DConditionModel(\n",
       "  (conv_in): Conv2d(12, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (time_proj): Timesteps()\n",
       "  (time_embedding): TimestepEmbedding(\n",
       "    (linear_1): LoRACompatibleLinear(in_features=320, out_features=1280, bias=True)\n",
       "    (act): SiLU()\n",
       "    (linear_2): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
       "  )\n",
       "  (down_blocks): ModuleList(\n",
       "    (0): CrossAttnDownBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-1): 2 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "          (proj_in): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
       "                (to_v): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): LoRACompatibleLinear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): LoRACompatibleLinear(in_features=768, out_features=320, bias=False)\n",
       "                (to_v): LoRACompatibleLinear(in_features=768, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): LoRACompatibleLinear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (conv1): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
       "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample2D(\n",
       "          (conv): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): CrossAttnDownBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-1): 2 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "          (proj_in): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
       "                (to_v): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): LoRACompatibleLinear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): LoRACompatibleLinear(in_features=768, out_features=640, bias=False)\n",
       "                (to_v): LoRACompatibleLinear(in_features=768, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): LoRACompatibleLinear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (conv1): LoRACompatibleConv(320, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): LoRACompatibleConv(320, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (conv1): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample2D(\n",
       "          (conv): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): CrossAttnDownBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-1): 2 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (proj_in): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_v): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): LoRACompatibleLinear(in_features=768, out_features=1280, bias=False)\n",
       "                (to_v): LoRACompatibleLinear(in_features=768, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (conv1): LoRACompatibleConv(640, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): LoRACompatibleConv(640, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (downsamplers): ModuleList(\n",
       "        (0): Downsample2D(\n",
       "          (conv): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): DownBlock2D(\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up_blocks): ModuleList(\n",
       "    (0): UpBlock2D(\n",
       "      (resnets): ModuleList(\n",
       "        (0-2): 3 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "          (conv1): LoRACompatibleConv(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): LoRACompatibleConv(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (upsamplers): ModuleList(\n",
       "        (0): Upsample2D(\n",
       "          (conv): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): CrossAttnUpBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-2): 3 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "          (proj_in): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_v): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
       "                (to_k): LoRACompatibleLinear(in_features=768, out_features=1280, bias=False)\n",
       "                (to_v): LoRACompatibleLinear(in_features=768, out_features=1280, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0-1): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 2560, eps=1e-05, affine=True)\n",
       "          (conv1): LoRACompatibleConv(2560, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): LoRACompatibleConv(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "          (conv1): LoRACompatibleConv(1920, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
       "          (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): LoRACompatibleConv(1920, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (upsamplers): ModuleList(\n",
       "        (0): Upsample2D(\n",
       "          (conv): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): CrossAttnUpBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-2): 3 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 640, eps=1e-06, affine=True)\n",
       "          (proj_in): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
       "                (to_v): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): LoRACompatibleLinear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): LoRACompatibleLinear(in_features=640, out_features=640, bias=False)\n",
       "                (to_k): LoRACompatibleLinear(in_features=768, out_features=640, bias=False)\n",
       "                (to_v): LoRACompatibleLinear(in_features=768, out_features=640, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): LoRACompatibleLinear(in_features=640, out_features=640, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((640,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): LoRACompatibleLinear(in_features=640, out_features=5120, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): LoRACompatibleLinear(in_features=2560, out_features=640, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): LoRACompatibleConv(640, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1920, eps=1e-05, affine=True)\n",
       "          (conv1): LoRACompatibleConv(1920, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): LoRACompatibleConv(1920, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "          (conv1): LoRACompatibleConv(1280, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): LoRACompatibleConv(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "          (conv1): LoRACompatibleConv(960, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=640, bias=True)\n",
       "          (norm2): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): LoRACompatibleConv(960, 640, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (upsamplers): ModuleList(\n",
       "        (0): Upsample2D(\n",
       "          (conv): LoRACompatibleConv(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): CrossAttnUpBlock2D(\n",
       "      (attentions): ModuleList(\n",
       "        (0-2): 3 x Transformer2DModel(\n",
       "          (norm): GroupNorm(32, 320, eps=1e-06, affine=True)\n",
       "          (proj_in): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (transformer_blocks): ModuleList(\n",
       "            (0): BasicTransformerBlock(\n",
       "              (norm1): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn1): Attention(\n",
       "                (to_q): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
       "                (to_v): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): LoRACompatibleLinear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm2): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (attn2): Attention(\n",
       "                (to_q): LoRACompatibleLinear(in_features=320, out_features=320, bias=False)\n",
       "                (to_k): LoRACompatibleLinear(in_features=768, out_features=320, bias=False)\n",
       "                (to_v): LoRACompatibleLinear(in_features=768, out_features=320, bias=False)\n",
       "                (to_out): ModuleList(\n",
       "                  (0): LoRACompatibleLinear(in_features=320, out_features=320, bias=True)\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (norm3): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (ff): FeedForward(\n",
       "                (net): ModuleList(\n",
       "                  (0): GEGLU(\n",
       "                    (proj): LoRACompatibleLinear(in_features=320, out_features=2560, bias=True)\n",
       "                  )\n",
       "                  (1): Dropout(p=0.0, inplace=False)\n",
       "                  (2): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (proj_out): LoRACompatibleConv(320, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (resnets): ModuleList(\n",
       "        (0): ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 960, eps=1e-05, affine=True)\n",
       "          (conv1): LoRACompatibleConv(960, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
       "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): LoRACompatibleConv(960, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1-2): 2 x ResnetBlock2D(\n",
       "          (norm1): GroupNorm(32, 640, eps=1e-05, affine=True)\n",
       "          (conv1): LoRACompatibleConv(640, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=320, bias=True)\n",
       "          (norm2): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (conv2): LoRACompatibleConv(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (nonlinearity): SiLU()\n",
       "          (conv_shortcut): LoRACompatibleConv(640, 320, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mid_block): UNetMidBlock2DCrossAttn(\n",
       "    (attentions): ModuleList(\n",
       "      (0): Transformer2DModel(\n",
       "        (norm): GroupNorm(32, 1280, eps=1e-06, affine=True)\n",
       "        (proj_in): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (transformer_blocks): ModuleList(\n",
       "          (0): BasicTransformerBlock(\n",
       "            (norm1): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn1): Attention(\n",
       "              (to_q): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_v): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_out): ModuleList(\n",
       "                (0): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm2): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn2): Attention(\n",
       "              (to_q): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=False)\n",
       "              (to_k): LoRACompatibleLinear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_v): LoRACompatibleLinear(in_features=768, out_features=1280, bias=False)\n",
       "              (to_out): ModuleList(\n",
       "                (0): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (norm3): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (ff): FeedForward(\n",
       "              (net): ModuleList(\n",
       "                (0): GEGLU(\n",
       "                  (proj): LoRACompatibleLinear(in_features=1280, out_features=10240, bias=True)\n",
       "                )\n",
       "                (1): Dropout(p=0.0, inplace=False)\n",
       "                (2): LoRACompatibleLinear(in_features=5120, out_features=1280, bias=True)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (proj_out): LoRACompatibleConv(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (resnets): ModuleList(\n",
       "      (0-1): 2 x ResnetBlock2D(\n",
       "        (norm1): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "        (conv1): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (time_emb_proj): LoRACompatibleLinear(in_features=1280, out_features=1280, bias=True)\n",
       "        (norm2): GroupNorm(32, 1280, eps=1e-05, affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (conv2): LoRACompatibleConv(1280, 1280, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (nonlinearity): SiLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_norm_out): GroupNorm(32, 320, eps=1e-05, affine=True)\n",
       "  (conv_act): SiLU()\n",
       "  (conv_out): Conv2d(320, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "vae.requires_grad_(False)\n",
    "unet.requires_grad_(True)\n",
    "text_encoder.requires_grad_(False)\n",
    "text_encoder.text_model.embeddings.token_embedding.requires_grad_(False)\n",
    "\n",
    "vae.eval()\n",
    "text_encoder.text_model.embeddings.token_embedding.eval()\n",
    "text_encoder.eval()\n",
    "unet.train()\n",
    "\n",
    "vae.to(device)\n",
    "unet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "model = FinetuneUnet(unet=unet, vae=vae, scheduler=scheduler, text_encoder=text_encoder, tokenizer=tokenizer)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 12, 32, 32)\n",
      "(1000, 12, 32, 32)\n",
      "(1000, 12, 32, 32)\n",
      "<class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "render = np.load('./render_depth_normal_latent.npy').astype(np.float32)\n",
    "print(render.shape)\n",
    "# render = render.transpose(0, 3, 1, 2)\n",
    "print(render.shape)\n",
    "# data = render[0:5000]\n",
    "data = render\n",
    "# data = np.concatenate([render,render,render],axis=1)\n",
    "print(data.shape)\n",
    "print(type(data[0][1][1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(333, torch.Size([3, 12, 32, 32]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    dataset=data,\n",
    "    batch_size=3,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "len(loader), next(iter(loader)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1000 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show(images,save=False,name='default'):\n",
    "    from matplotlib import pyplot as plt\n",
    "\n",
    "    if type(images) == torch.Tensor:\n",
    "        images = images.to('cpu').detach().numpy()\n",
    "\n",
    "    images = images[:50]\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        image = images[i]\n",
    "        image = image.transpose(1, 2, 0)\n",
    "        # image = (image + 1) / 2\n",
    "        if image.shape[2] == 1:\n",
    "            plt.subplot(5, 10, i + 1)\n",
    "            plt.imshow(image,cmap='gray')\n",
    "            plt.axis('off')\n",
    "            \n",
    "        else:\n",
    "            plt.subplot(5, 10, i + 1)\n",
    "            plt.imshow(image)\n",
    "            plt.axis('off')\n",
    "    if save:\n",
    "        plt.savefig(f'{name}.pdf') \n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def show_error_map(pred, gd, save=False,name='default'):\n",
    "    # Ensure the tensors have the same shape\n",
    "    assert pred.shape == gd.shape, \"Input tensors must have the same shape\"\n",
    "    pred = pred.mean(dim=1, keepdim=True)\n",
    "    gd = gd.mean(dim=1, keepdim=True)\n",
    "    # Calculate the absolute difference between the prediction and ground truth\n",
    "    error_map = torch.abs(pred - gd) * 10\n",
    "    show(error_map,save,name)\n",
    "show(next(iter(loader))[:,2:3,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 12, 32, 32])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(loader)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "def ssim_loss(data,pred):\n",
    "    ssim_values = 0.0  # Initialize a variable to accumulate SSIM values\n",
    "    # Loop through the batch of images\n",
    "    data = data.detach().cpu().numpy().transpose(0, 2, 3, 1)\n",
    "    pred = pred.detach().cpu().numpy().transpose(0, 2, 3, 1)\n",
    "    for i in range(data.shape[0]):\n",
    "        # Calculate SSIM between original and reconstructed image\n",
    "        # ssim_val = ssim(upscaled_img1, upscaled_img2, multichannel=True, channel_axis=2)\n",
    "        ssim_val = ssim(data[i], pred[i], multichannel=True, data_range=data[i].max()-data[i].min())\n",
    "        ssim_values += ssim_val\n",
    "    \n",
    "    # Compute the average SSIM across all images in the batch\n",
    "    average_ssim = ssim_values / data.shape[0]\n",
    "    loss = 1 - average_ssim\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from skimage.metrics import structural_similarity as ssim\n",
    "# def train():\n",
    "#     criterion = torch.nn.L1Loss()\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(),\n",
    "#                                   lr=1e-5,                   \n",
    "#                                   weight_decay=1e-4)\n",
    "#     model.to(device)\n",
    "#     model.train()\n",
    "\n",
    "#     accu_loss  = 0\n",
    "#     total_loss = 0 \n",
    "#     for epoch in range(2000):\n",
    "#         for i, data in enumerate(loader):\n",
    "#             pred_noise, noise = model(data.to(device))\n",
    "#             loss = criterion(pred_noise, noise)\n",
    "#             loss = loss + 5 * ssim_loss(pred_noise, noise)\n",
    "#             accu_loss += loss\n",
    "#             total_loss += loss\n",
    "#             if (i+1) % 8 == 0:\n",
    "#                 accu_loss.backward()\n",
    "#                 torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "#                 optimizer.step()\n",
    "#                 optimizer.zero_grad()\n",
    "#                 print(f\"Epoch {epoch + 1}, Batch {i + 1}/{len(loader)}, Loss: {accu_loss.item():.4f}\", end='\\r')\n",
    "#                 accu_loss = 0\n",
    "#             torch.cuda.empty_cache()\n",
    "#         if epoch % 1 == 0:\n",
    "#             print(epoch, total_loss)\n",
    "#             total_loss = 0\n",
    "#             # model.eval()\n",
    "#             # show(generate(10, device))\n",
    "#             # model.train()\n",
    "#             torch.save(unet.state_dict(), './unet_finetune.pth')\n",
    "\n",
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 142.72917690873146 0.14272917690873146\n",
      "1 48.04192288964987 0.04804192288964987\n",
      "2 37.251867569983006 0.037251867569983006\n",
      "3 30.877938348799944 0.030877938348799944\n",
      "4 27.973741300404072 0.027973741300404073\n",
      "5 25.341458212584257 0.025341458212584256\n",
      "6 25.440560776740313 0.025440560776740312\n",
      "7 24.248304419219494 0.024248304419219494\n",
      "8 22.861088152974844 0.022861088152974843\n",
      "9 23.587552811950445 0.023587552811950446\n",
      "10 23.94608622789383 0.02394608622789383\n",
      "11 22.565422037616372 0.022565422037616373\n",
      "12 20.96161136776209 0.02096161136776209\n",
      "13 21.228751327842474 0.021228751327842472\n",
      "14 20.451470382511616 0.020451470382511616\n",
      "15 20.962827725335956 0.020962827725335954\n",
      "16 20.91096622683108 0.020910966226831078\n",
      "17 19.373019525781274 0.019373019525781274\n",
      "18 18.289206640794873 0.018289206640794874\n",
      "19 18.189612183719873 0.018189612183719873\n",
      "20 18.778424635529518 0.01877842463552952\n",
      "21 17.22875402122736 0.01722875402122736\n",
      "22 17.78722302056849 0.01778722302056849\n",
      "23 17.66657606512308 0.01766657606512308\n",
      "24 16.92632320523262 0.01692632320523262\n",
      "25 18.877098770812154 0.018877098770812152\n",
      "26 16.880970826372504 0.016880970826372504\n",
      "27 15.872208684682846 0.015872208684682847\n",
      "28 16.46700384095311 0.01646700384095311\n",
      "29 16.875416113063693 0.016875416113063695\n",
      "30 16.32769904471934 0.016327699044719337\n",
      "Epoch 32, Batch 99/333, Loss: 0.0294\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 31\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[38;5;66;03m# model.eval()\u001b[39;00m\n\u001b[1;32m     27\u001b[0m             \u001b[38;5;66;03m#show(generate(10, device))\u001b[39;00m\n\u001b[1;32m     28\u001b[0m             \u001b[38;5;66;03m# model.train()\u001b[39;00m\n\u001b[1;32m     29\u001b[0m             torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./unet_finetune.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 22\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     21\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mprint\u001b[39m(epoch, loss_sum, loss_sum\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "def train():\n",
    "    criterion = torch.nn.L1Loss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                                  lr=4e-5,                   \n",
    "                                  weight_decay=1e-4)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    loss_sum = 0\n",
    "    for epoch in range(1000):\n",
    "        for i, data in enumerate(loader):\n",
    "            pred_noise, noise = model(data.to(device))\n",
    "            loss = criterion(pred_noise, noise)\n",
    "            # loss = loss + 1 * ssim_loss(pred_noise, noise)\n",
    "\n",
    "            loss.backward()\n",
    "            loss_sum += loss.item()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            print(f\"Epoch {epoch + 1}, Batch {i + 1}/{len(loader)}, Loss: {loss.item():.4f}\", end='\\r')\n",
    "        if epoch % 1 == 0:\n",
    "            print(epoch, loss_sum, loss_sum/1000)\n",
    "            loss_sum = 0\n",
    "            # model.eval()\n",
    "            #show(generate(10, device))\n",
    "            # model.train()\n",
    "            torch.save(model.state_dict(), './unet_finetune.pth')\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ssim_values = 0.0  # Initialize a variable to accumulate SSIM values\n",
    "            # # Loop through the batch of images\n",
    "            # data = data.detach().cpu().numpy().transpose(0, 2, 3, 1)\n",
    "            # pred = pred.detach().cpu().numpy().transpose(0, 2, 3, 1)\n",
    "            # for i in range(data.shape[0]):\n",
    "            #     # Calculate SSIM between original and reconstructed image\n",
    "                \n",
    "            #     # ssim_val = ssim(upscaled_img1, upscaled_img2, multichannel=True, channel_axis=2)\n",
    "            #     ssim_val = ssim(data[i], pred[i], multichannel=True)\n",
    "            #     ssim_values += ssim_val\n",
    "            \n",
    "            # # Compute the average SSIM across all images in the batch\n",
    "            # average_ssim = ssim_values / data.shape[0]\n",
    "            # loss = 1 * loss + 0 * (1 - average_ssim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = torch.nn.L1Loss()\n",
    "# optimizer = torch.optim.AdamW(model.parameters(),\n",
    "#                                   lr=5e-5,\n",
    "#                                   weight_decay=1e-4)\n",
    "# def train():\n",
    "#     for epoch in range(2000):\n",
    "#         for i, data in enumerate(loader):\n",
    "            \n",
    "#             pred_noise, noise = model(data.to(device))\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             loss = criterion(pred_noise, noise)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             print(f\"Epoch {epoch + 1}, Batch {i + 1}/{len(loader)}, Loss: {loss.item():.4f}\", end='\\r')\n",
    "#         torch.save(model.state_dict(), './unet_finetune.pth')\n",
    "\n",
    "\n",
    "# train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------------TEST CODE -----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model.to(device)\n",
    "# model.eval()\n",
    "# unet = model.unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data.shape)\n",
    "# data = data[0:4,:,:,:]\n",
    "# data = torch.from_numpy(data).to(device)\n",
    "# print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_noise, noise = model(data)\n",
    "# print(pred_noise.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depth_latent = pred_noise[:,0:4,:,:]\n",
    "# depth_latent.shape\n",
    "# output_tensor = torch.mean(depth_latent, dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rgb_latent_scale_factor = 0.18215\n",
    "# depth_latent_scale_factor = 0.18215\n",
    "# def encode_3c(render: torch.Tensor) -> torch.Tensor:\n",
    "#         # encode\n",
    "#         h = vae.encoder(render)\n",
    "#         moments = vae.quant_conv(h)\n",
    "#         mean, logvar = torch.chunk(moments, 2, dim=1)\n",
    "#         # scale latent\n",
    "#         render_latent = mean * rgb_latent_scale_factor\n",
    "#         return render_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# render = np.load('./render.npy').astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(render.shape)\n",
    "# render = render.transpose(0,3,1,2)\n",
    "# render = torch.from_numpy(render).cuda()\n",
    "# print(render.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae = vae.cuda()\n",
    "# out = encode_3c(render[3:7])\n",
    "# print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# data_all = np.empty((5400,4,32,32),np.float32)\n",
    "# for i in tqdm(range(data_all.shape[0])):\n",
    "#     out = encode_3c(render[i:i+1])\n",
    "#     data_all[i] = out.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# render_depth_normal_latent = np.concatenate([data_all,data_all,data_all],axis=1)\n",
    "# render_depth_normal_latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('./render_depth_normal_latet.npy',render_depth_normal_latent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marigold",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
